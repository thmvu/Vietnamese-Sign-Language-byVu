import streamlit as st
import numpy as np
import tensorflow as tf
import tempfile
import os
import cv2
import mediapipe as mp
from scipy.interpolate import interp1d
import time
st.set_page_config(page_title="VSL Prediction", layout="centered")
st.title("Dá»° ÄOÃN NGÃ”N NGá»® KÃ HIá»†U")

mp_holistic = mp.solutions.holistic
N_UPPER_BODY_POSE_LANDMARKS = 25
N_HAND_LANDMARKS = 21
N_TOTAL_LANDMARKS = N_UPPER_BODY_POSE_LANDMARKS + N_HAND_LANDMARKS + N_HAND_LANDMARKS

ALL_POSE_CONNECTIONS = list(mp_holistic.POSE_CONNECTIONS)
UPPER_BODY_POSE_CONNECTIONS = []
# ====================
# Load model vÃ  label_map
# ====================
@st.cache_resource
def load_model():
    return tf.keras.models.load_model(
        'Models/checkpoints/final_model.keras',
        compile=False
    )  # model Ä‘Ã£ huáº¥n luyá»‡n

@st.cache_data
def load_label_map():
    import json
    with open('Logs/label_map.json', 'r', encoding='utf-8') as f:
        label_map = json.load(f)
    inv_label_map = {v: k for k, v in label_map.items()}
    return label_map, inv_label_map

model = load_model()
label_map, inv_label_map = load_label_map()
# ====================
# HÃ m xá»­ lÃ½ video (placeholder)
# ====================
def mediapipe_detection(image, model):
    # Mediapipe dÃ¹ng RGB, cv2 dÃ¹ng BGR
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results

def extract_keypoints(results):
    pose_kps = np.zeros((N_UPPER_BODY_POSE_LANDMARKS, 3))
    left_hand_kps = np.zeros((N_HAND_LANDMARKS, 3))
    right_hand_kps = np.zeros((N_HAND_LANDMARKS, 3))
    if results and results.pose_landmarks:
        for i in range(N_UPPER_BODY_POSE_LANDMARKS):
            if i < len(results.pose_landmarks.landmark):
                res = results.pose_landmarks.landmark[i]
                pose_kps[i] = [res.x, res.y, res.z]
    if results and results.left_hand_landmarks:
        left_hand_kps = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark])
    if results and results.right_hand_landmarks:
        right_hand_kps = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark])
    keypoints = np.concatenate([pose_kps,left_hand_kps, right_hand_kps])
    return keypoints.flatten()

def interpolate_keypoints(keypoints_sequence, target_len = 60):#ná»™i suy chuá»—i keypoints vá» 60 frames
    if len(keypoints_sequence) == 0:
        return None

    original_times = np.linspace(0, 1, len(keypoints_sequence))
    target_times = np.linspace(0, 1, target_len)

    num_features = keypoints_sequence[0].shape[0]
    interpolated_sequence = np.zeros((target_len, num_features))

    for feature_idx in range(num_features):
        feature_values = [frame[feature_idx] for frame in keypoints_sequence]

        interpolator = interp1d(
            original_times, feature_values,
            kind='cubic', #ná»™i suy cubic
            bounds_error=False, #khÃ´ng bÃ¡o lá»—i náº¿u ngoÃ i pháº¡m vi
            fill_value="extrapolate" #ngoáº¡i suy náº¿u cáº§n
        )
        interpolated_sequence[:, feature_idx] = interpolator(target_times)

    return interpolated_sequence

def sequence_frames(video_path, holistic):
  sequence_frames = []
  cap = cv2.VideoCapture(video_path)
  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

  step = max(1, total_frames // 100)  # xÃ¡c Ä‘á»‹nh bÆ°á»›c nháº£y Ä‘á»ƒ láº¥y máº«u frames

  while cap.isOpened():#Ä‘á»c tá»«ng frame tá»« video
      ret, frame = cap.read()
      if not ret:
          break

      #náº¿u khÃ´ng pháº£i frame cáº§n láº¥y máº«u thÃ¬ bá» qua
      if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % step != 0:
          continue

      try:
          image, results = mediapipe_detection(frame, holistic)#dÃ¹ng mediapipe Ä‘á»ƒ xÃ¡c Ä‘á»‹nh keypoints
          keypoints = extract_keypoints(results)#trÃ­ch xuáº¥t keypoints tá»« káº¿t quáº£

          if keypoints is not None:
              sequence_frames.append(keypoints)

      except Exception as e:
          continue

  cap.release()
  return sequence_frames

def process_webcam_to_sequence():
    cap = cv2.VideoCapture(0)  # Sá»­ dá»¥ng webcam máº·c Ä‘á»‹nh
    st.write("â³ Äang chuáº©n bá»‹... Báº¯t Ä‘áº§u trong 1.5 giÃ¢y...")
    time.sleep(1.5)  # Hiá»ƒn thá»‹ thÃ´ng bÃ¡o trong 1.5 giÃ¢y
    
    # Äá»c video tá»« webcam trong 4 giÃ¢y
    st.write("ğŸ¥ Äang ghi hÃ¬nh trong 4 giÃ¢y...")
    sequence = []
    start_time = time.time()

    # Khá»Ÿi táº¡o Mediapipe Holistic model
    holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)
    stframe = st.empty()

    while True:
        ret, frame = cap.read()
        if not ret:
            st.error("KhÃ´ng thá»ƒ truy cáº­p webcam")
            break
        elapsed_time = time.time() - start_time
        if elapsed_time > 4:  # Sau 4 giÃ¢y thÃ¬ dá»«ng
            break
        # Chuyá»ƒn Ä‘á»•i frame tá»« BGR (OpenCV) sang RGB (Mediapipe)
        image, results = mediapipe_detection(frame, holistic)

        # TrÃ­ch xuáº¥t keypoints tá»« káº¿t quáº£ cá»§a Mediapipe
        keypoints = extract_keypoints(results)
        
        # ThÃªm keypoints vÃ o chuá»—i (cÃ³ thá»ƒ dá»«ng sau 60 frames hoáº·c khi ngÆ°á»i dÃ¹ng nháº¥n nÃºt)
        if keypoints is not None:
            sequence.append(keypoints)

        # Hiá»ƒn thá»‹ webcam feed trÃªn Streamlit
        stframe.image(image, channels="BGR", caption="Webcam feed", use_container_width=True)

    cap.release()
    
    return sequence

# Streamlit App

input_mode = st.radio("Chá»n nguá»“n Ä‘áº§u vÃ o:", ["ğŸï¸ Video file", "ğŸ“· Webcam"])

sequence = None
holistic =mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)
if input_mode == "ğŸï¸ Video file":
    uploaded_file = st.file_uploader("Táº£i lÃªn video (.mp4, .avi)", type=["mp4", "avi"])
    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_file.read())
            tmp_path = tmp.name
        st.video(tmp_path)
        if st.button("ğŸ” Dá»± Ä‘oÃ¡n tá»« video"):
            sequence = sequence_frames(tmp_path, holistic)

elif input_mode == "ğŸ“· Webcam":
    st.warning("Nháº¥n nÃºt bÃªn dÆ°á»›i Ä‘á»ƒ báº¯t Ä‘áº§u ghi hÃ¬nh tá»« webcam.")
    if st.button("ğŸ“¸ Ghi vÃ  dá»± Ä‘oÃ¡n"):
        sequence = process_webcam_to_sequence()

# Dá»± Ä‘oÃ¡n
if sequence is not None:
    kp = interpolate_keypoints(sequence)
    result = model.predict(np.expand_dims(kp, axis=0))
    pred_idx = np.argmax(result, axis=1)
    pred_label = [inv_label_map[idx] for idx in pred_idx]
    st.success(f"âœ… NhÃ£n dá»± Ä‘oÃ¡n: **{pred_label}**")
